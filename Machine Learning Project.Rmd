---
title: "Machine Learning Project"
author: "Kim Aquilino"
date: "Tuesday, December 02, 2014"
output: html_document
---

```{r}
#setwd("C://Users//kaquilin//Documents//Class//Machine Learning//")
library(caret)
```

## Objective

The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to classify if their barbell lifts were performed correctly and if not, classify the error based on common mistakes. 

## Data Source
 
For the training data, six participants were asked to perform one set of 10 repetitions of bicep curls correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). We will use this data to develop a prediction model. More information on the dataset is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). The study paper is cited here:

>  Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

The data sets are [train](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). Here we download the datasets, load the training data and set/coerce to the proper classes.

```{r}
train.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!file.exists("train.csv")) download.file(train.url, "train.csv")
if(!file.exists("test.csv")) download.file(test.url, "test.csv")

column.classes <- c("character","factor","integer","integer","character","factor","integer",rep("character",152),"factor")

train <- read.csv("train.csv",
                  colClasses=column.classes,
                  na.strings=c('""',"NA","#DIV/0!"))
# convert meaasures to numeric
for(i in c(8:ncol(train)-1)) {
    train[,i] <- as.numeric(train[,i])
}
dim(train)
```

## Pre-Process Data

Let's start by partitioning the _training_ data set into _training_ and _validation_ partitions. This will allow us to see how the results of our prediction model will generalize to an independent data set. By default, the random sampling is done within the classes, since it is a factor, in an attempt to balance the class distributions within the splits. 

```{r}
set.seed(4567)
in.train <- createDataPartition(y=train$classe,p=.75,list=FALSE)
training <- train[in.train,]
validation <- train[-in.train,]
dim(training)
dim(validation)
colnames(training)
```

Variables in the training dataset with all NA or 0 values will be removed. The training data includes derived features (mean, variance, etc.) for sliding windows of time. These values are populated on overservations where `new_window=1`. Since the test set only provides specific instances of time, we will not be able to derive these sliding window features or use them as features and therefore will remove them from the training set. We will also remove non-measurement variables such as times, particpant's name, and window references. The training set is reduced from 160 columns to 53.

> For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and
magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 derived feature sets.

```{r}
# identify columns where all values are 0 or NA, excluding rows whose new_window value is "yes"
cols <- colnames(training)
nacols <- c()
for(i in cols) {
    if (sum(!(is.na(training[,i]) | training[,i] == 0 | training[,"new_window"] == "yes")) == 0) {
       nacols <- c(nacols,i) 
       }
}
training <- training[,!names(training) %in% nacols] # remove the NA/0 columns
training <- training[-c(1:7)] # remove the times, participant, and window variables 
dim(training)
```

## Data Summary & Exploration
Here is a summary of our tidy dataset and a feature plot showing how each measurement relates to the `classe` variable. We see that there is an outlier in the gyroscope measurements for the dumbbell and and forearm sensors. It is identified as a single observation and removed from the dataset. 

```{r}
colnames(training)
summary(training)

featurePlot(x=training[,1:51],
            y=training$classe,
            plot="box",
            scales=list(y=list(relation="free")),
            auto.key=list(columns=3))
training[training$gyros_dumbbell_x<(-200),]

# remove the outlier seen on the forearm and dumbbell and gyroscope
training <- training[!training$gyros_dumbbell_x<(-200),]
dim(training)
featurePlot(x=training[,1:51],
            y=training$classe,
            plot="box",
            scales=list(y=list(relation="free")),
            auto.key=list(columns=3))

```

We are ready to train the model. Since it is a categorical outcome (or Classification model), we will use the random forest method with maximum accuracy (fraction correct) for optimal model selection. 

```{r}
set.seed(1235)
fit <- train(classe ~ .,data=training,method="rf",metric="Accuracy",maximize=TRUE)
fit
```

Let's take a look at the important variables. For random forests, importance is defined as follows in the R package:

> "For each tree, the prediction accuracy on the out-of-bag portion of the data is recorded. Then the same is done after permuting each predictor variable. The difference between the two accuracies are then averaged over all trees, and normalized by the standard error. For regression, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. If the standard error is equal to 0 for a variable, the division is not done." 

The most important variables determined by the `train` function are shown below.
```{r}
varImp(fit, scale=FALSE)
plot(varImp(fit,scale=FALSE,top=20))
```

Let's predict on the validation set and check the results by looking at the confusion matrix and overall statistics.

```{r}
predictions <- predict(fit,newdata=validation)
confusionMatrix(predictions,validation$classe) #expected out of sample error rate
error.rate <- sum(predictions!=validation$classe)/length(predictions) * 100
```

The expected out of sample error rate is `{r} error.rate`.

#str(train)
# convert raw timestamp to a new datetime variable and remove the original date variables
#train$datetime <- as.POSIXct(train$raw_timestamp_part_1,origin="1970-01-01")
## Background

The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 


You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

## For k-fold cross validation

To minimize variability, we will use k-fold cross validation. With this method, the data set is randomly partitioned into `{r} k=10` subsets of equal size. The cross validation is repeated `{r}k` times, each time using a different subset as the _validation_ set and all others as the _training_ set. The validation results are then averaged. 

```{r}
set.seed(4567)
folds <- createFolds(y=train$classe,k=10)
sapply(folds,length)
```

fit$finalModel$confusion

fit$times
fit$control
fit$times
fit$finalModel
fit$bestTune
