---
title: "Machine Learning Project"
author: "Kim Aquilino"
date: "Friday, December 19, 2014"
output:
  html_document:
    fig_width: 8
    theme: flatly
---

```{r echo=FALSE, warning=FALSE,message=FALSE}
#setwd("C://Users//kaquilin//Documents//Class//Machine Learning//")
library(caret)
```

****

## Objective

The goal of this project is to use data from accelerometers positioned on the belt, forearm, arm, and dumbell of 6 study participants to classify if their barbell lifts were performed correctly and if not, classify the error based on common mistakes. 

## Data Source

The data for this project is described in the Weight Lifting Exercise Dataset section at http://groupware.les.inf.puc-rio.br/har. 

For the training data, the six participants were asked to perform one set of 10 repetitions of bicep curls correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). We will use this data to develop a machine learning algorithm for predicting activity quality.

There are two data sets provided for the course project: [train](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [test](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). Here we download the datasets, load the training data and coerce the measurement variables to numeric.

```{r}
train.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!file.exists("train.csv")) download.file(train.url, "train.csv")
if(!file.exists("test.csv")) download.file(test.url, "test.csv")

train <- read.csv("train.csv", na.strings=c('""',"NA","#DIV/0!"))
for(i in c(8:ncol(train)-1)) train[,i] <- as.numeric(train[,i])
dim(train)
```

## Pre-Processing

We partition the training data set into _training_ and _validation_ partitions using a 75/25% split so that we may perform cross-validation. This will allow us to see how the results of our prediction model will generalize to an independent data set. 

```{r}
set.seed(4567)
in.train <- createDataPartition(y=train$classe,p=.75,list=FALSE)
training <- train[in.train,]
validation <- train[-in.train,]
dim(training); dim(validation)
```

The training data includes derived aggregate features (mean, variance, etc.) for sliding windows of time (as quoted below) which are populated in the dataset on observations where `new_window = 1`. Since the test data _does not_ include measurements over windows of time, I chose to remove these features for model training.

> For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 derived feature sets.

Variables in the training dataset with all NA or 0 values and non-measurement variables (times, name, window references) are removed, reducing the training set from 160 variables to 53.

```{r}
# identify columns where all values are 0 or NA or the new_window value is "yes"
cols <- colnames(training)
nacols <- c()
for(i in cols) {
    if (sum(!(is.na(training[,i]) | training[,i] == 0 | 
                 training[,"new_window"] == "yes")) == 0) {
       nacols <- c(nacols,i) 
       }
}
training <- training[,!names(training) %in% nacols] # remove the NA/0 columns
training <- training[-c(1:7)] # remove the times, participant, and window variables 
dim(training)
```

## Data Summary & Exploration
Here is a summary of our tidy dataset showing the min, max, and mean values for each variable.

```{r fig.height=5,fig.width=9}
cbind(min=round(sapply(training[-53],min),2),max=round(sapply(training[-53],max),2),mean=round(sapply(training[-53],mean),2))
```


A boxplot of the gyroscope on both the dumbbell and forearm in the x direction reflects an outlier that is well out of the range of all other values. It is a single observation and I chose to remove it from the dataset.  

```{r echo=FALSE,fig.height=4, fig.width=8}
par(mfrow=c(1,2))
boxplot(gyros_dumbbell_x ~ classe,data=training,xlab="classe",ylab="gyros_dumbbell_x")
boxplot(gyros_forearm_x ~ classe,data=training,xlab="classe",ylab="gyros_forearm_x")
training <- training[!training$gyros_dumbbell_x<(-200),]
dim(training)
```

Here is a feature plot showing how each measurement relates to the `classe` variable. 

```{r fig.height=10, fig.width=8}
par(mar=c(3,2,2,1))
featurePlot(x=training[,1:52],
            y=training$classe,
            plot="box",
            scales=list(y=list(relation="free")),
            layout=c(5,11))
```

## Model Training

We train a classification model to predict the `classe` outcome using the random forest method with maximum accuracy (fraction correct) for optimal model selection. 

```{r cache=TRUE,warning=FALSE,message=FALSE}
set.seed(1235)
fit <- train(classe ~ .,data=training,method="rf",metric="Accuracy",maximize=TRUE)
fit
```

The model results show 98.9% accuracy. Let's take a look at the most important variables. For random forests, importance is defined in the R package as follows:

> For each tree, the prediction accuracy on the out-of-bag portion of the data is recorded. Then the same is done after permuting each predictor variable. The difference between the two accuracies are then averaged over all trees, and normalized by the standard error. For regression, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. If the standard error is equal to 0 for a variable, the division is not done.

The most important features determined by the `train` function are shown below. The roll measurement on the belt contributes most to the model, among all features in the training set. 
```{r fig.height=7,fig.width=8,echo=FALSE, warning=FALSE,message=FALSE}
plot(varImp(fit,scale=FALSE,top=20))
```

## Model Evaluation

Let's predict on the validation set and check the results by looking at the confusion matrix and overall statistics.

```{r}
predictions <- predict(fit,newdata=validation)
confusionMatrix(predictions,validation$classe) #expected out of sample error rate
error.rate <- sum(predictions!=validation$classe)/length(predictions) * 100
error.rate
```

The predictions on the validation set are 99% accurate. The expected out of sample error rate is `r round(error.rate,2)`%. This model appears to perform well and will be used to predict the `classe` on the test data.

```{r echo=FALSE}
test <- read.csv("test.csv", na.strings=c('""',"NA","#DIV/0!"))
for(i in c(8:ncol(test)-1)) {           # convert measures to numeric
    test[,i] <- as.numeric(test[,i])
}
answers <- predict(fit,test)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```